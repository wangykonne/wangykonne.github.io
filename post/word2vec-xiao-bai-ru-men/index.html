<html>
<head>
    <meta charset="utf-8" />
<meta name="description" content="" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<title>Word2vec小白入门 | db自留地</title>

<link rel="shortcut icon" href="https://wangykonne.github.io/favicon.ico?v=1615345247936">

<link href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://wangykonne.github.io/styles/main.css">
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/css/bootstrap.min.css"> -->

<style>
    hr {
        margin-top: 1rem;
        margin-bottom: 1rem;
        border: 0;
        border-top: 1px solid rgba(0, 0, 0, 0.1);
    }
</style>

<script src="https://cdn.jsdelivr.net/npm/@highlightjs/cdn-assets/highlight.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dockerfile.min.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.15.10/languages/dart.min.js"></script>

<!-- <script src="https://cdn.jsdelivr.net/npm/moment@2.27.0/moment.min.js"></script> -->
<!-- <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js"></script> -->
<!-- <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"></script> -->
<!-- <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.0/dist/js/bootstrap.min.js"></script> -->
<!-- DEMO JS -->
<!--<script src="media/scripts/index.js"></script>-->


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.css">
</head>
<body>
<div class="main gt-bg-theme-color-first">
    <style>
    /* 导航栏样式 */
    .navbar {
        position: relative;
        display: -ms-flexbox;
        display: flex;
        -ms-flex-wrap: wrap;
        flex-wrap: wrap;
        -ms-flex-align: center;
        align-items: center;
        -ms-flex-pack: justify;
        justify-content: space-between;
        padding: 0.5rem 1rem;
    }

    .navbar-brand {
        display: inline-block;
        padding-top: 0.3125rem;
        padding-bottom: 0.3125rem;
        margin-right: 1rem;
        font-size: 1.25rem;
        line-height: inherit;
        white-space: nowrap;
    }

    .navbar-brand:hover,
    .navbar-brand:focus {
        text-decoration: none;
    }

    .navbar-nav {
        display: -ms-flexbox;
        display: flex;
        -ms-flex-direction: column;
        flex-direction: column;
        padding-left: 0;
        margin-bottom: 0;
        list-style: none;
    }

    .navbar-collapse {
        -ms-flex-preferred-size: 100%;
        flex-basis: 100%;
        -ms-flex-positive: 1;
        flex-grow: 1;
        -ms-flex-align: center;
        align-items: center;
    }

    .navbar-toggler {
        padding: 0.25rem 0.75rem;
        font-size: 1.25rem;
        line-height: 1;
        background-color: transparent;
        border: 1px solid transparent;
        border-radius: 0.25rem;
    }

    .navbar-toggler:hover,
    .navbar-toggler:focus {
        text-decoration: none;
    }

    @media (min-width: 992px) {
        .navbar-expand-lg {
            -ms-flex-flow: row nowrap;
            flex-flow: row nowrap;
            -ms-flex-pack: start;
            justify-content: flex-start;
        }

        .navbar-expand-lg .navbar-nav {
            -ms-flex-direction: row;
            flex-direction: row;
        }

        .navbar-expand-lg .navbar-collapse {
            display: -ms-flexbox !important;
            display: flex !important;
            -ms-flex-preferred-size: auto;
            flex-basis: auto;
        }

        .navbar-expand-lg .navbar-toggler {
            display: none;
        }
    }

    @media (max-width: 991px) {
        #navbarSupportedContent {
            display: none;
        }
    }
</style>
<nav class="navbar navbar-expand-lg">
    <div class="navbar-brand">
        <img class="user-avatar" src="/images/avatar.png" alt="头像">
        <div class="site-name gt-c-content-color-first">
            db自留地
        </div>
    </div>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
        aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <i class="fas fa-bars gt-c-content-color-first" style="font-size: 18px"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <div class="navbar-nav mr-auto" style="text-align: center">
            
            <div class="nav-item">
                
                <a href="/" class="menu gt-a-link">
                    首页
                </a>
                
            </div>
            
            <div class="nav-item">
                
                <a href="/archives" class="menu gt-a-link">
                    归档
                </a>
                
            </div>
            
            <div class="nav-item">
                
                <a href="/tags" class="menu gt-a-link">
                    标签
                </a>
                
            </div>
            
            <div class="nav-item">
                
                <a href="/post/about" class="menu gt-a-link">
                    关于
                </a>
                
            </div>
            
        </div>
        <div style="text-align: center">
            <form id="gridea-search-form" style="position: relative" data-update="1615345247936"
                action="/search/index.html">
                <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="搜索文章" />
                <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
        </div>
    </div>
</nav>
<script>
    /* 移动端导航栏展开/收起切换 */
    document.getElementById('changeNavbar').onclick = function () {
        var element = document.getElementById('navbarSupportedContent');
        if (element.style.display === 'none' || element.style.display === '') {
            element.style.display = 'block';
        } else {
            element.style.display = 'none';
        }
    }
</script>
    <div class="post-container">
        <div class="post-detail gt-bg-theme-color-second">
            <article class="gt-post-content">
                <h2 class="post-title">
                    Word2vec小白入门
                </h2>
                <div class="post-info">
                    <time class="post-time gt-c-content-color-first">
                        · 2021-01-18 ·
                    </time>
                    
                        <a href="https://wangykonne.github.io/tag/oCxH4CJK4/" class="post-tags">
                            # 机器学习理论
                        </a>
                    
                </div>
                <div class="post-content">
                    <blockquote>
<p>一些才疏学浅的见解</p>
</blockquote>
<!-- more -->
<p>  最近实习在做的是一个类似NLP的模型，然而此前在学校并没有了解过NLP的相关概念，所以工作中也是现学现卖。前几天终于把初版的模型跑出来了，因此先把最近学习到的东西写下来防止以后忘了。</p>
<h2 id="词的向量化方法">词的向量化方法</h2>
<p>  在介绍Word2vec之前，有必要了解一下两种词向量化的方法：①One-hot；②Distributed。（可能不止这两种，但目前我只看了这两种方法）</p>
<h3 id="one-hot表示法">One-hot表示法</h3>
<p>  One-hot表示法之前就有接触过，简单的说，这种方法经常用于离散的类别化的特征转换，例如：公司旁边的奶茶店有CoCo、七分甜、一点点，那就可以把奶茶店这个特征进行One-hot编码，用[1,0,0]表示CoCo，[0,1,0]表示七分甜，[0,0,1]表示一点点。虽然这样做会将特征升维，但这样可以将离散的类别化特征变得可处理。（毕竟单纯用0,1,2表示类别是不太合理的，因为这样就隐含了大小信息）</p>
<p>  同样，在词向量的表示上，也有One-hot编码方法。假设一个词库中有N个词语，那one-hot就会为每一个词语分配一个索引，使之转化为一个N维向量。举个例子，假设词库中现在有：Today、is、Monday、Tuesday、Sunny五个词，那Today is Monday这句话经过One-hot表示法转化为词向量就变为[1,1,1,0,0]，非常易懂。</p>
<p>  但这种方法其实存在不足。首先，一般词库是非常大的，而一句话中所能包含的词语有限，所以这种方法生成的词向量往往是稀疏的，在词库很大很大的情况下容易造成维数灾难；其次，一句话中的词语之间往往也是有关系的，而one-hot表示法不能体现这一点，它仅仅是记录每个词语是否出现或出现多少次，即Today is Monday和Is today monday?是一样的表示方法，但显然两者的含义是不一样的。</p>
<h3 id="distributed表示法">Distributed表示法</h3>
<p>  Distributed表示法是一种通过神经网络将句子中的词语训练为限定维数K的向量的方法，且这种方法得到的向量是稠密的，不仅仅能记录词语，还能表示上下文词语的关系。唯一需要考虑的就是如何选取恰当的K，一般要参考词库的大小。</p>
<h2 id="word2vec简介">Word2vec简介</h2>
<h3 id="什么是word2vec">什么是Word2vec？</h3>
<p>  用一句通俗的话概括，Word2vec就是采用Distributed表示法，将原本是one-hot形式的稀疏词向量表示为一个指定维数n的稠密词向量的方法。词语是NLP的最小维度的单位，如何将符号性质的词语转化为数值形式就衍生出了词嵌入方法（word embedding），即将词语嵌入到指定维数的数学空间中，本质上，Word2vec就是一种词嵌入方法。Word2vec要做的就是分析一句话中的某个词语与前后m个词语的关系，或者是词语x与词语y放在一起是否符合自然语言的法则。在实际应用中，我们并不关心Word2vec模型的结果怎么样，而是更关注训练出模型的参数并用来转化一些现实的句子。</p>
<h3 id="cbow和skip-gram">CBOW和Skip-Gram</h3>
<p>  CBOW（Continuous Bag-of-Word Model）又称为连续词袋模型，整体是一个三层神经网络，该模型的特点是通过输入某个词前后的一段语料或者词库，输出对该词的预测结果。如下图所示：<br>
<img src="https://wangykonne.github.io/post-images/1610979281941.png" alt="" loading="lazy"></p>
<p>  对某个特定的词语，显然CBOW预测词语为w时必然也是后验概率<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>w</mi><mi mathvariant="normal">∣</mi><mi>C</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>(</mo><mi>w</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">P(w|Context(w))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">x</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span>最大的时候，所以对于一段语料或者词库中所有的词语，可以得到，CBOW的学习目标就是最大化对数似然函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>(</mo><mi>w</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">L(w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span>：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>(</mo><mi>w</mi><mo>)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>w</mi><mo>∈</mo><mi>C</mi></mrow></munder><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo>(</mo><mi>w</mi><mi mathvariant="normal">∣</mi><mi>C</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>(</mo><mi>w</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">L(w)=\sum_{w\in C}logP(w|Context(w))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3717110000000003em;vertical-align:-1.321706em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8556639999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.321706em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">x</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p>  加上一个负号即得到损失函数，为最优化问题的目标函数：</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>(</mo><mi>w</mi><mo>)</mo><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mrow><mi>w</mi><mo>∈</mo><mi>C</mi></mrow></munder><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mo>(</mo><mi>w</mi><mi mathvariant="normal">∣</mi><mi>C</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>(</mo><mi>w</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">Loss(w)=-\sum_{w\in C}logP(w|Context(w))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3717110000000003em;vertical-align:-1.321706em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8556639999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.321706em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault">x</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p>  下面这张图描绘了了CBOW的训练过程：<br>
<img src="https://wangykonne.github.io/post-images/1610979381232.jpg" alt="" loading="lazy"></p>
<p>  简单解释一下就是：①首先，在Input layer中，我们考虑&quot;某个词&quot;前后共C个词语的one-hot表示的向量，假设维数为1*V（V表示词库或语料的总维数），那么对于这C个1*V的向量，通过神经网络训练一个词语的权值矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>V</mi><mo>∗</mo><mi>N</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{V*N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span><span class="mbin mtight">∗</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，这里N就是我们想要的词向量的维数；②用这C个1*V的向量分别与权值矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>V</mi><mo>∗</mo><mi>N</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{V*N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span><span class="mbin mtight">∗</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>相乘得到C个1*N的向量，在hidden layer中取平均得到一个1*N的向量；③最后用hidden layer中的这个1*N的向量再与权值矩阵的转置<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">W^{T}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span>相乘，变回一个1*V的向量。</p>
<p>  此时得到的这个1*V的向量的每个分量表示的就是对应词语在&quot;某个词&quot;这个位置上的概率值，概率最大的词语就是Word2vec模型预测出的中间词。</p>
<p>  Skip-Gram则是恰好与CBOW的思路相反，它的输入是词库或语料中的一个词语，输出则是这个词语前后共C个词语，因此Skip-Gram的训练过程类似CBOW，只是过程相反，就不多说了，具体类似下图：<br>
<img src="https://wangykonne.github.io/post-images/1610979461752.jpg" alt="" loading="lazy"></p>
<h3 id="some-tricks">Some Tricks</h3>
<p>  不管是CBOW还是Skip-Gram，我们需要通过神经网络训练的都是权值矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>V</mi><mo>∗</mo><mi>N</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{V*N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span><span class="mbin mtight">∗</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，共V*N个待训练参数，然而在实际应用中这是个规模非常大的矩阵，首先word2vec方法内置的默认N=1000即使用1000维的词向量，那么如果待训练的词库有10000个词则就有10000000个待训练参数，这将会是非常慢的，同时这么大的权重矩阵就需要更多的训练样本来防止出现过拟合现象。因此，Word2vec内置了两种优化的训练方法：</p>
<p>  ①层序softmax（hierarchical softmax）方法</p>
<p>  用CBOW举例，在上面提到的训练过程中，从hidden layer的1*N的向量到output layer的1*V的概率向量的过程中，激活函数是softmax函数，但这样的话我们需要计算V次softmax函数的值来得到所有词语在中间位出现的概率值。而使用层序softmax方法，我们只需要计算<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub><mi>V</mi></mrow><annotation encoding="application/x-tex">log_2V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>次，这相当于从多项式时间复杂度变为对数时间复杂度。具体的计算过程如下图：<br>
<img src="https://wangykonne.github.io/post-images/1610979523483.jpg" alt="" loading="lazy"></p>
<p>  类比CBOW的output层得到的全部V个词的概率，在上面这个霍夫曼树中表现为V个叶子节点；而由权重矩阵投影后的1*N词向量在树中表现为根节点<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>(</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mn>1</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">n(w_2,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>；而隐藏层的一些神经元则体现为树的内部节点。直观上看，此时原来神经网络中的一步softmax转化为霍夫曼树中沿不同父节点进行一次又一次的sigmoid转化，这就是hierarchical softmax的直观体现。具体细节可以理解为：首先定义向左分支为1，向右分支为0（相反也行），此时方向就可以看成一个逻辑回归，这样判别一个给定的词w就转化为沿着父节点一次次进行sigmoid函数的计算。</p>
<p>  这种方法的好处很明显：首先降低了时间复杂度；其次如果词库或语料中有高频词汇，其在霍夫曼树中会更加靠近根节点的位置（因为出现的概率大），这样就能更快寻找到正确的词w。</p>
<p>  ②负采样（negative sampling）方法</p>
<p>  层序softmax方法虽然能显著降低算法的时间复杂度，但也有一些不足，即如果我们要训练的是一个很生僻的词语，则需要在霍夫曼树中往下深入很多。而负采样方法的思想能一定程度上优化这个不足，它将词库或者语料中一些出现频率非常高的词作为negative samples，而频率非常高的词一般没有什么实际意义（例如英文中的the和汉语中的&quot;的&quot;）。本质上，这种方法实际上是对训练集进行采样来减少训练集的大小。</p>
<p>  简单地说下流程就是：①首先选择neg个negative samples出来；②对中间位的前后C个词，只使用这neg个negative samples和1个正例样本进行训练，即由原来的权值矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>V</mi><mo>∗</mo><mi>N</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{V*N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span><span class="mbin mtight">∗</span><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>变为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>V</mi><mo>∗</mo><mo>(</mo><mn>1</mn><mo>+</mo><mi>n</mi><mi>e</mi><mi>g</mi><mo>)</mo></mrow></msub></mrow><annotation encoding="application/x-tex">W_{V*(1+neg)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.03853em;vertical-align:-0.3551999999999999em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.34480000000000005em;"><span style="top:-2.5198em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.22222em;">V</span><span class="mbin mtight">∗</span><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3551999999999999em;"><span></span></span></span></span></span></span></span></span></span>，这就大大简化了模型待训练的参数。（插点个人的理解，这种方法不应该会丢失很多信息吗？感觉是一种非常偷懒的方法...不过也可能是我没理解透这个方法的含义吧）</p>
<p>  最后，如何选择negative samples呢？用一种形象的表述就是：如果词库或者语料vocab的词总数为V，那么我们就将一段长度为1的线段分成V份，每份对应词汇表中的一个词。当然每个词对应的线段长度是不一样的，高频词对应的线段长，低频词对应的线段短。每个词w的线段长度由下式决定:</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>e</mi><mi>n</mi><mo>(</mo><mi>w</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>(</mo><mi>w</mi><mo>)</mo></mrow><mrow><munder><mo>∑</mo><mrow><mi>u</mi><mo>∈</mo><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi></mrow></munder><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>(</mo><mi>u</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">len(w)=\frac{count(w)}{\sum_{u\in vocab}count(u)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.44008em;vertical-align:-1.01308em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.18639799999999984em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.32708000000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">u</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault">u</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">u</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.01308em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>  在某paper上已经有大佬给出了调试好的公式，对分子分母都取了3/4次幂:</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>e</mi><mi>n</mi><mo>(</mo><mi>w</mi><mo>)</mo><mo>=</mo><mfrac><msup><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>(</mo><mi>w</mi><mo>)</mo></mrow><mrow><mn>3</mn><mi mathvariant="normal">/</mi><mn>4</mn></mrow></msup><msup><mrow><munder><mo>∑</mo><mrow><mi>u</mi><mo>∈</mo><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi></mrow></munder><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>(</mo><mi>u</mi><mo>)</mo></mrow><mrow><mn>3</mn><mi mathvariant="normal">/</mi><mn>4</mn></mrow></msup></mfrac></mrow><annotation encoding="application/x-tex">len(w)=\frac{{count(w)}^{3/4}}{{\sum_{u\in vocab}count(u)}^{3/4}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.9498800000000003em;vertical-align:-1.24498em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7049em;"><span style="top:-2.11em;"><span class="pstrut" style="height:3.0279em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.18639799999999984em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">u</span><span class="mrel mtight">∈</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.32708000000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">u</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault">u</span><span class="mclose">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0279em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">/</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.2579em;"><span class="pstrut" style="height:3.0279em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.7049em;"><span class="pstrut" style="height:3.0279em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">u</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0279em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">/</span><span class="mord mtight">4</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.24498em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<h3 id="word2vec其实不关注模型">Word2vec其实不关注模型</h3>
<p>  Word2vec模型可以分为两部分：模型与模型训练得到的权值矩阵W。</p>
<p>  正如标题所言，大部分情况下我们对Word2vec模型本身并不感兴趣，真正有用的是通过训练后模型内的权值矩阵W，得到这个权值矩阵W后，我们就可以将任意一段词语在词库中的句子或文章进行向量化了。简而言之，Word2vec只是一种工具，我们需要的只是模型的中间产物，最后的模型结果判断中间词之类的一般用不着（因为语料中我们已经知道真实词是哪一个了）</p>
<h2 id="总结">总结</h2>
<p>  以上就是我上周对Word2vec一些内容的学习。总体还是学的挺浅的，毕竟是目标导向的学习，没有深入了解算法背后的一些数学原理比如怎么最小化损失函数啊之类的，嗯以后有机会或许会补充（如果看得懂的话）。如果之后mentor没有布置任务的话，还有些TFIDF相关的内容和工作中写代码踩过的雷可以更新。</p>

                </div>
            </article>
        </div>

        
            <div class="next-post">
                <div class="next gt-c-content-color-first">下一篇</div>
                <a href="https://wangykonne.github.io/post/po-shi-san/" class="post-title gt-a-link">
                    破事儿氵
                </a>
            </div>
        

        

        

        
            <script src='https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js'></script>

<style>
	div#vcomments{
		width:100%;
		max-width: 1000px;
		padding: 2.5%
	}
</style>


	<div id="vcomments"></div>

<script>
	new Valine({
		el: '#vcomments',
		appId: 'aHb7wrt4LdY1y2xNnPnv8Bdf-9Nh9j0Va',
		appKey: '3ND7IFEdq0etn28alvpNtWLk',
		avatar: 'mp',
		pageSize: 5,
		recordIp: false,
		placeholder: 'Just Go Go',
		visitor: false,
	});
</script>

        

        <div class="site-footer gt-c-content-color-first">
    <div class="slogan gt-c-content-color-first">日近长安远</div>
    <div class="social-container">
        
            
        
            
        
            
        
            
        
            
        
            
        
    </div>
    <div class="footer-info">
        Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
    </div>
    <div>
        Theme by <a href="https://imhanjie.com/" target="_blank">imhanjie</a>, Powered by <a
                href="https://github.com/getgridea/gridea" target="_blank">Gridea | <a href="https://wangykonne.github.io/atom.xml" target="_blank">RSS</a></a>
    </div>
</div>

<script>
  hljs.initHighlightingOnLoad()
</script>

    </div>
</div>
</body>
</html>
